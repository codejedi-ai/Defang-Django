{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo Vision: Window-based and Scan-line Stereo Matching\n",
    "\n",
    "Complete implementation of stereo vision algorithms for 3D reconstruction from stereo pairs using the Tsukuba dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements two main approaches to stereo vision:\n",
    "\n",
    "1. **Window-based Stereo**: Traditional local aggregation methods\n",
    "2. **Scan-line Stereo with Viterbi**: Advanced dynamic programming approaches with regularization\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Uses the classic **Tsukuba dataset** - one of the oldest stereo datasets with dense ground-truth disparity maps, produced at University of Tsukuba in 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as image\n",
    "from matplotlib.colors import LogNorm\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_image(img):\n",
    "    return np.cumsum(np.cumsum(img, axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SD_array(imageL, imageR, d_minimum, d_maximum):\n",
    "    imageL_float = imageL.astype(np.float64)\n",
    "    imageR_float = imageR.astype(np.float64)\n",
    "    SD = np.zeros((1+d_maximum-d_minimum, np.shape(imageL)[0], np.shape(imageL)[1]))\n",
    "    for i in range(d_minimum, 1+d_maximum):\n",
    "        im_right_roll = np.roll(imageR_float, i, axis=1)\n",
    "        SD_slice = LA.norm(imageL_float - im_right_roll, axis=2) ** 2\n",
    "        SD[i - d_minimum,:,:] = SD_slice\n",
    "    return SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_7b(im_left, im_right, d_min, d_max, w, window_size):\n",
    "    drange = d_max - d_min + 1\n",
    "    height, width = im_left.shape[:2]\n",
    "    E_bar = np.full((drange, height, width), np.inf)\n",
    "    SD_array_image = SD_array(im_left, im_right, d_min, d_max)\n",
    "    \n",
    "    for d in range(drange):\n",
    "        smoothed_SD = windSum(SD_array_image[d], window_size)\n",
    "        finite_mask = np.isfinite(smoothed_SD)\n",
    "        if np.any(finite_mask):\n",
    "            mean_val = np.mean(smoothed_SD[finite_mask])\n",
    "            std_val = np.std(smoothed_SD[finite_mask])\n",
    "            penalty = mean_val + 3 * std_val\n",
    "            smoothed_SD[~finite_mask] = penalty\n",
    "        else:\n",
    "            smoothed_SD = np.full_like(smoothed_SD, 1000.0)\n",
    "        SD_array_image[d] = smoothed_SD\n",
    "    \n",
    "    for d in range(drange):\n",
    "        E_bar[d, :, 0] = SD_array_image[d, :, 0]\n",
    "    \n",
    "    for col in range(1, width):\n",
    "        for d_curr in range(drange):\n",
    "            for row in range(height):\n",
    "                data_cost = SD_array_image[d_curr, row, col]\n",
    "                min_transition_cost = np.inf\n",
    "                for d_prev in range(drange):\n",
    "                    transition_cost = w * abs(d_curr - d_prev)\n",
    "                    total_cost = E_bar[d_prev, row, col-1] + transition_cost\n",
    "                    min_transition_cost = min(min_transition_cost, total_cost)\n",
    "                E_bar[d_curr, row, col] = data_cost + min_transition_cost\n",
    "    \n",
    "    disparity_map = np.zeros((height, width), dtype=int)\n",
    "    for row in range(height):\n",
    "        disparity_map[row, -1] = np.argmin(E_bar[:, row, -1])\n",
    "    \n",
    "    for col in range(width-2, -1, -1):\n",
    "        for row in range(height):\n",
    "            d_curr = disparity_map[row, col+1]\n",
    "            min_cost = np.inf\n",
    "            best_d_prev = 0\n",
    "            for d_prev in range(drange):\n",
    "                transition_cost = w * abs(d_curr - d_prev)\n",
    "                total_cost = E_bar[d_prev, row, col] + transition_cost\n",
    "                if total_cost < min_cost:\n",
    "                    min_cost = total_cost\n",
    "                    best_d_prev = d_prev\n",
    "            disparity_map[row, col] = best_d_prev\n",
    "    \n",
    "    return disparity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "DATA = \"/kaggle/input/stereodatascenes2001datatsukuba\"\n",
    "\n",
    "# you should use this random dot stereo pair for code developing/testing in Probelms 1-5\n",
    "im_left = image.imread(os.path.join(DATA, \"images/stereo_pairs/rds_left.gif\"))\n",
    "im_right = image.imread(os.path.join(DATA, \"images/stereo_pairs/rds_right.gif\"))\n",
    "\n",
    "fig = plt.figure(figsize = (12, 5))\n",
    "plt.subplot(121)\n",
    "plt.title(\"left image (reference)\")\n",
    "plt.imshow(im_left)\n",
    "plt.subplot(122)\n",
    "plt.title(\"right image\")\n",
    "plt.imshow(im_right)\n",
    "\n",
    "# the range of disparities for this random dot stereo pair \n",
    "d_min = 0\n",
    "d_max = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Squared Difference Computation\n",
    "\n",
    "Compute the squared L2 norm between RGB pixels of the left and right images.\n",
    "\n",
    "**Key Implementation Details:**\n",
    "- Convert RGB values to `float64` to prevent overflow bugs\n",
    "- Use L2 norm: $D = ||I_L - I_R||^2$\n",
    "- Essential first step for all stereo algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD = np.zeros(np.shape(im_left)) \n",
    "SD = LA.norm(im_left - im_right, axis=2) \n",
    "\n",
    "fig = plt.figure(figsize = (12, 5))\n",
    "plt.title(\"SD between left and right images\")\n",
    "plt.imshow(SD, cmap = \"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Squared Difference Array Function\n",
    "\n",
    "Compute squared differences for **all disparity values** $d \\in [d_{min}, d_{max}]$ by shifting the right image horizontally.\n",
    "\n",
    "**Algorithm:**\n",
    "- Shift right image by $d$ pixels for each disparity value\n",
    "- Compute L2 squared difference: $SD[d] = ||I_L - shift(I_R, d)||^2$\n",
    "- Return 3D array where $SD[i, r, c]$ contains squared difference for disparity $i$ at pixel $(r,c)$\n",
    "\n",
    "**Disparity Range:** Typically $d \\in [0, 15]$ for Tsukuba dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = range(1, 90)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Squared Differences\n",
    "\n",
    "Display squared difference images for minimum, middle, and maximum disparity values to understand how the matching behaves across the disparity range.\n",
    "\n",
    "**Insight:** \n",
    "- Low disparity values (background) show different features than high disparity values (foreground objects)\n",
    "- This visualization helps understand the matching behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD = SD_array(im_left, im_right, d_min, d_max)    \n",
    "\n",
    "fig = plt.figure(figsize = (15, 4))\n",
    "plt.subplot(131)\n",
    "plt.title('SD for d_min = {:>2d}'.format(d_min))\n",
    "plt.imshow(SD[0], cmap = \"gray\")\n",
    "plt.subplot(132) \n",
    "d_middle = round((d_min+d_max)/2)\n",
    "plt.title('SD for d_middle = {:>2d}'.format(d_middle))\n",
    "plt.imshow(SD[d_middle-d_min], cmap = \"gray\")\n",
    "plt.subplot(133)\n",
    "plt.title('SD for d_max = {:>2d}'.format(d_max))\n",
    "plt.imshow(SD[d_max-d_min], cmap = \"gray\")\n",
    "#plt.colorbar(cax=plt.axes([0.91, 0.25, 0.01, 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Integral Image Function\n",
    "\n",
    "Compute the **integral image** (summed area table) for efficient window operations.\n",
    "\n",
    "**Why Integral Images?**\n",
    "- Enable $O(1)$ window sum queries\n",
    "- Critical for fast SSD computation  \n",
    "- Foundation for all window-based stereo methods\n",
    "\n",
    "**Formula:** \n",
    "\n",
    "$$I(x,y) = \\sum_{i=0}^{x} \\sum_{j=0}^{y} img(i,j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Integral Images for All Disparities\n",
    "\n",
    "Apply the integral image function to each disparity level to prepare for efficient window sum computation.\n",
    "\n",
    "This creates the foundation for **Sum of Squared Differences (SSD)** - the core metric for window-based stereo matching.\n",
    "\n",
    "$$SSD_d = I(x,y) = \\sum_{i=0}^{x} \\sum_{j=0}^{y} SD[d](i,j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_SD = np.zeros(np.shape(SD))\n",
    "print(np.shape(integral_SD),np.shape(SD))\n",
    "\n",
    "for Delta in range(1+d_max-d_min):\n",
    "    integral_SD[Delta] = integral_image(SD[Delta])\n",
    "        \n",
    "fig = plt.figure(figsize = (15, 4))\n",
    "plt.subplot(131)\n",
    "plt.title('integral SD for d_min = {:>2d}'.format(d_min))\n",
    "plt.imshow(integral_SD[0], cmap = \"gray\")\n",
    "plt.subplot(132) \n",
    "d_middle = round((d_min+d_max)/2)\n",
    "plt.title('integral SD for d_middle = {:>2d}'.format(d_middle))\n",
    "plt.imshow(integral_SD[d_middle-d_min], cmap = \"gray\")\n",
    "plt.subplot(133)\n",
    "plt.title('integral SD for d_max = {:>2d}'.format(d_max))\n",
    "plt.imshow(integral_SD[d_max-d_min], cmap = \"gray\")\n",
    "#plt.colorbar(cax=plt.axes([0.91, 0.2, 0.01, 0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Window Sum Function\n",
    "\n",
    "Compute **Sum of Squared Differences (SSD)** by aggregating values within fixed-size windows using integral images.\n",
    "\n",
    "**Key Features:**\n",
    "- Works with any window size $h \\in \\{1,2,3,4,5,\\ldots\\}$\n",
    "- Uses integral images for $O(1)$ window sum computation\n",
    "- Margins filled with $\\infty$ to mark invalid regions\n",
    "\n",
    "**Window Size Effects:**\n",
    "- Small windows ($h=1-3$): More detail, more noise\n",
    "- Large windows ($h=15-20$): Smoother results, less detail\n",
    "\n",
    "**Window Sum Formula:**\n",
    "\n",
    "$$SSD_d(x,y) = I(x+h/2, y+h/2) - I(x-h/2, y+h/2) - I(x+h/2, y-h/2) + I(x-h/2, y-h/2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_integral_image(x2,a,b):\n",
    "    x3 = np.roll(x2, (a,b), axis=(0,1))\n",
    "    return x3\n",
    "def shift_integral_image_2(x2,a,b):\n",
    "    x3 = np.roll(x2, (-a,-b), axis=(0,1))\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function windSum can be applied to any scalar 2D array/image. It should return an array/image where the value of \n",
    "# each element (pixel p) is the \"sum\" of the values in the input array/image within a window around element p.\n",
    "# The return image should be of the same size/type and have its margins (around half-window width) filled with INFTY.\n",
    "# NOTE: you should use function integral_image implemented earlier.\n",
    "# HINT: you should use standard np.roll function to avoid double or triple for loops.\n",
    "INFTY = np.inf\n",
    "\n",
    "def windSum(img, window_width):\n",
    "    img_integral = integral_image(img)\n",
    "    # Roll the x and y values by - math.floor((window_width - 1) / 2). Which \n",
    "    f = img_integral - shift_integral_image(img_integral,0,window_width) - shift_integral_image(img_integral,window_width,0) + shift_integral_image(img_integral,window_width,window_width)\n",
    "    shift_1 = int(np.floor( (window_width - 1) / 2))\n",
    "    shift_2 = int(np.ceil(window_width / 2))\n",
    "    sample = (np.ones(img.shape) * np.inf)\n",
    "    core_center_need_padding_image = f[window_width - 1:,window_width - 1:]\n",
    "    sample[shift_1 :shift_1 + core_center_need_padding_image.shape[0],shift_1 :shift_1 + core_center_need_padding_image.shape[1]] = core_center_need_padding_image\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_down(x2,a,b):\n",
    "    x3 = np.roll(x2, (a,b), axis=(0,1))\n",
    "    return x3\n",
    "def left_up(x2,a,b):\n",
    "    x3 = np.roll(x2, (-a,-b), axis=(0,1))\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing SSD for Multiple Window Sizes\n",
    "\n",
    "Compare SSD images for different window sizes to understand the trade-off between detail and smoothness.\n",
    "\n",
    "**Expected Results:**\n",
    "- Window size = 1: Maximum detail, raw squared differences\n",
    "- Window size > 1: Smoother but less detailed results\n",
    "\n",
    "This demonstrates the fundamental trade-off in stereo vision: **detail vs. smoothness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD1 = np.zeros(np.shape(SD))\n",
    "SSD2 = np.zeros(np.shape(SD))\n",
    "SSD5 = np.zeros(np.shape(SD))\n",
    "\n",
    "for Delta in range(1+d_max-d_min):\n",
    "    SSD1[Delta] = windSum(SD[Delta],1)\n",
    "    SSD2[Delta] = windSum(SD[Delta],2)\n",
    "    SSD5[Delta] = windSum(SD[Delta],5)\n",
    "\n",
    "\n",
    "d_middle = round((d_min+d_max)/2)\n",
    "        \n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "plt.subplot(331)\n",
    "plt.title('SSD for window=1,   d_min={:>2d}'.format(d_min))\n",
    "plt.imshow(1+SSD1[0], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(332) \n",
    "plt.title('SSD for window=1,   d_middle = {:>2d}'.format(d_middle))\n",
    "plt.imshow(1+SSD1[d_middle-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(333)\n",
    "plt.title('SSD for window=1,   d_max = {:>2d}'.format(d_max))\n",
    "plt.imshow(1+SSD1[d_max-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(334)\n",
    "plt.title('SSD for window=2,   d_min = {:>2d}'.format(d_min))\n",
    "plt.imshow(1+SSD2[0], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(335) \n",
    "plt.title('SSD for window=2,   d_middle = {:>2d}'.format(d_middle))\n",
    "plt.imshow(1+SSD2[d_middle-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(336)\n",
    "plt.title('SSD for window=2,   d_max = {:>2d}'.format(d_max))\n",
    "plt.imshow(1+SSD2[d_max-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(337)\n",
    "plt.title('SSD for window=5,   d_min = {:>2d}'.format(d_min))\n",
    "plt.imshow(1+SSD5[0], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(338) \n",
    "plt.title('SSD for window=5,   d_middle = {:>2d}'.format(d_middle))\n",
    "plt.imshow(1+SSD5[d_middle-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(339)\n",
    "plt.title('SSD for window=5,   d_max = {:>2d}'.format(d_max))\n",
    "plt.imshow(1+SSD5[d_max-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Disparity Map from SSD\n",
    "\n",
    "Compute the final **disparity map** by selecting the disparity with minimum SSD at each pixel.\n",
    "\n",
    "**Algorithm:**\n",
    "$$\\text{disparity}(x,y) = \\arg\\min_d SSD_d(x,y)$$\n",
    "\n",
    "**Key Points:**\n",
    "- At each pixel $(x,y)$, find $d$ that minimizes $SSD_d(x,y)$\n",
    "- For margin pixels (with $SSD = \\infty$), set disparity to $0$\n",
    "- This is the **Winner-Takes-All** approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should use functions np.where (pointwise \"if, then, else\" operation) and np.minimum (pointwise \"minimum\" operation)\n",
    "# These functions will help to avoid double loops for traversing the pixels.\n",
    "# WARNING: there will be a deducton for double-loops traversing pixels, but single loop to traverse disparities is OK.\n",
    "# for all parts that have zero error apply the i and all parts that is non-zero apply \n",
    "def SSDtoDmap(SSD_array, d_minimum, d_maximum):\n",
    "    # print(SSD_array.shape)\n",
    "    dMap = np.full(np.shape(SSD_array[0]),d_minimum)\n",
    "    dMap [SSD_array[0] == INFTY] = 0\n",
    "    SSD_minimum = SSD_array[0]\n",
    "    dMap [:,:] = 0\n",
    "    for i in range(1+d_maximum-d_minimum):\n",
    "        dMap [SSD_array[i] < SSD_minimum] = i\n",
    "        SSD_minimum = np.minimum(SSD_minimum, SSD_array[i])\n",
    "    return dMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparity Map Visualization\n",
    "\n",
    "Display disparity maps computed with different window sizes to compare the effects of aggregation.\n",
    "\n",
    "**Observations:**\n",
    "- Larger windows produce smoother disparity maps\n",
    "- Smaller windows preserve more details but have more noise\n",
    "- Window size is a critical parameter for balancing quality and detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMap1 = SSDtoDmap(SSD1,d_min,d_max)\n",
    "dMap2 = SSDtoDmap(SSD2,d_min,d_max)\n",
    "dMap5 = SSDtoDmap(SSD5,d_min,d_max)\n",
    "\n",
    "fig = plt.figure(figsize = (15, 3))\n",
    "plt.subplot(131)\n",
    "plt.title(\"estimated disparity map (window=1)\")\n",
    "plt.imshow(dMap1, vmin = 0, vmax = d_max)\n",
    "plt.subplot(132)\n",
    "plt.title(\"estimated disparity map (window=2)\")\n",
    "plt.imshow(dMap2, vmin = 0, vmax = d_max)\n",
    "plt.subplot(133)\n",
    "plt.title(\"estimated disparity map (window=5)\")\n",
    "plt.imshow(dMap5, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.25, 0.015, 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: test your code on a real stereo pair with ground truth (Tsukuba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added closing parentheses to all imread calls\n",
    "im_left = image.imread(os.path.join(DATA, \"images/stereo_pairs/tsukuba/scene1.row3.col3.ppm\"))\n",
    "im_gt = image.imread(os.path.join(DATA, \"images/stereo_pairs/tsukuba/truedisp.row3.col3.pgm\"))\n",
    "im_right = image.imread(os.path.join(DATA, \"images/stereo_pairs/tsukuba/scene1.row3.col4.ppm\"))\n",
    "im_right2 = image.imread(os.path.join(DATA, \"images/stereo_pairs/tsukuba/scene1.row3.col5.ppm\"))\n",
    "\n",
    "fig = plt.figure(figsize = (12, 10))\n",
    "plt.subplot(221)\n",
    "plt.title(\"left image (reference)\")\n",
    "plt.imshow(im_left)\n",
    "plt.subplot(222) \n",
    "plt.title(\"(scaled) ground truth disparity map \")\n",
    "plt.imshow(im_gt)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.557, 0.015, 0.3]))\n",
    "plt.subplot(223)\n",
    "plt.title(\"right image (smaller baseline)\")\n",
    "plt.imshow(im_right)\n",
    "plt.subplot(224)\n",
    "plt.title(\"right image (larger baseline)\")\n",
    "plt.imshow(im_right2)\n",
    "\n",
    "# Added plt.show() to display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the integer-valued ground truth image above represents scaled disparity values for the pixels in the reference (left) mage. The scale w.r.t. the smaller baseline right image ($\\text{im_right}$) is 16 and for the larger baseline image ($\\text{im_right2}$) is 8. Below, you should use the smaller-baseline right image ($\\text{im_right}$). \n",
    "\n",
    "### Problem 6a: Using ground truth disparity map, estimate the range of disparity values between pixels in the left image ($\\text{im_left}$) and the right image ($\\text{im_right}$) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: use standard functions to find min and max values in the ground truth disparity map. \n",
    "# You should ignore 0-valued margin!\n",
    "\n",
    "d_min = 0 # change me\n",
    "d_max = 12 # change me\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute squared differences using $SD\\_array$ function and visualize the results using logarithmic scale. Note that linear scale would make it hard to see smaller squared differences since there are many very large ones. (fully implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD = SD_array(im_left, im_right, d_min, d_max)    \n",
    "    \n",
    "fig = plt.figure(figsize = (15, 4))\n",
    "plt.subplot(131)\n",
    "plt.title('SD for d_min = {:>2d}'.format(d_min))\n",
    "plt.imshow(im_left)\n",
    "plt.imshow(SD[0], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(132) \n",
    "d_middle = round((d_min+d_max)/2)\n",
    "plt.title('SD for d_middle = {:>2d}'.format(d_middle))\n",
    "plt.imshow(SD[d_middle-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.subplot(133)\n",
    "plt.title('SD for d_max = {:>2d}'.format(d_max))\n",
    "plt.imshow(SD[d_max-d_min], cmap = \"gray\", norm=LogNorm(vmin=1, vmax=200000))\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.2, 0.01, 0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6b: Explain the differences you observe above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer:\n",
    "\n",
    "for $$dmin = 0$$ I mainly see the features of the background and for higher levles of disparity I have seen more of the features of the face and the lamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6c: Write function `Dmap_Windows` that returns disparity map from a given stereo pair (left and right image), specified disparity range, and window size. Your implementation should combine functions implemented and debugged earlier (`SD_array`, `windSum`, and `SSDtoDmap`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dmap_Windows(imageL, imageR, d_minimum, d_maximum, window_width):\n",
    "    SD = SD_array(imageL, imageR, d_minimum, d_maximum)   \n",
    "    SSD = np.zeros(np.shape(SD))\n",
    "\n",
    "    for Delta in range(1+d_maximum-d_minimum):\n",
    "        SSD[Delta] = windSum(SD[Delta],window_width)\n",
    "    dMap = SSDtoDmap(SSD, d_minimum, d_maximum)\n",
    "    return dMap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im_left = image.imread(os.path.join(DATA, \"images/stereo_pairs/rds_left.gif\"))\n",
    "im_right = image.imread(os.path.join(DATA, \"images/stereo_pairs/rds_right.gif\"))\n",
    "d_min = 0\n",
    "d_max = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and show disparity maps for Tsukuba using small and large windows. (fully implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispMap_small = Dmap_Windows(im_left, im_right, d_min, d_max, 4)\n",
    "dispMap_large = Dmap_Windows(im_left, im_right, d_min, d_max, 15)\n",
    "\n",
    "fig = plt.figure(figsize = (16, 7))\n",
    "plt.subplot(131)\n",
    "plt.title(\"disparity map (small windows)\")\n",
    "plt.imshow(dispMap_small, vmin = 0, vmax = d_max)\n",
    "plt.subplot(132)\n",
    "plt.title(\"disparity map (large windows)\")\n",
    "plt.imshow(dispMap_large, vmin = 0, vmax = d_max)\n",
    "plt.subplot(133) \n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(im_gt/16, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Scan-line stereo $\\quad \\leftarrow 30 \\text{ pts}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7(a): Program $Viterbi$ approach discussed in class and apply it to Tsukuba example. For the photo-consistency term of the loss function (objective function) you can use previously implemented $\\text{SD\\_array}$\n",
    "$$\n",
    "D_p(d) = |I_p - I_{p+d}|^2 \\quad \\longleftarrow \\quad \\text{SD\\_array}[d][p]\n",
    "$$\n",
    "That for every pixel $p$ defines the cost for every possible disparity value $d \\in \\{d_{min}, ..., d_{max}\\}$. The regularization term should be\n",
    "$$\n",
    "V_{pq}(d_p, d_q) = w|d_p - d_q|\n",
    "$$\n",
    "Where you can select some value for parameter $w$ empirically (start from $w \\approx 0$). Discuss the differences with the results of the window-based stereo above.\n",
    "\n",
    "### NOTE: You should implement $Viterbi$ optimization yourself - it was fully covered in class. Organize your code (e.g., add cells, introduce functions, write comments, etc.) as part of your mark will depend on clarity. The main iteration of the forward pass in Viterbi ($m^2$-complexity operation for each pair of neighboring pixels on a scan-line) can be implemented as a separate function. You can avoid double for-loops using functions like `np.where`, `np.minimum`, `np.square`, `np.ogrid` or others similar general \"vectorized\" functions in numpy that allow to avoid multi-loops over matrix (image) elements (pixels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every line/row on the right image I am going to assume $$V_{pq}(d_p,d_q) = w|d_p-d_q| = w|dispMap[p] - dispMap[p+1]|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lower values of $w$ the disparity within a scanline seems to be noisy. Which is understandable as differnces between neighboring pixels are not peanlized. Where as for values other than $0$ the line is smoother. This comes at a cost in the form of loss of details when $w$ is set to high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "i is the the disparity value of the previouse column and k is the current value\n",
    "this function returns a matrix where E[i,k] signifies the value at E_bar[k,p] if taken at i\n",
    "'''\n",
    "def D(SD_array_image, row, drange, p):\n",
    "    empty_grid = np.ogrid[0:drange,0:drange]\n",
    "    i = empty_grid[0]\n",
    "    k = empty_grid[1]\n",
    "    return SD_array_image[k, row, p]\n",
    "def V(drange):\n",
    "    empty_grid = np.ogrid[0:drange,0:drange]\n",
    "    i = empty_grid[0]\n",
    "    k = empty_grid[1]\n",
    "    return np.abs(k - i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_array(im_left, im_right, d_min, d_max)[np.array([0,1,2]), :, 9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(im_left, im_right, d_min, d_max, w):\n",
    "    drange = d_max - d_min + 1\n",
    "    # the depth, the pixels and the other rows in the image\n",
    "    E_bar = np.zeros((drange,im_left.shape[0],im_left.shape[1]))\n",
    "    #print(E_bar.shape)\n",
    "    SD_array_image = SD_array(im_left, im_right, d_min, d_max)\n",
    "    \n",
    "\n",
    "\n",
    "    for p in range(1,im_left.shape[1]):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        #candidate_arrays = E_bar[i,p - 1] + (SD_array_image[k, row, p] + w * np.abs(k - i))\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * w * V(drange)[:,:,np.newaxis])\n",
    "        #print(candidate_arrays.shape)\n",
    "        \n",
    "        \n",
    "        #print(np.min(candidate_arrays,axis = 0).shape)\n",
    "        # One need to take the minimum value along axis 0 to get the minimum values of i that needs to be taken for each k\n",
    "        E_bar[k, :, p] = np.min(candidate_arrays,axis = 0)    \n",
    "    \n",
    "    #print(E_bar)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    p = im_left.shape[1] - 1\n",
    "\n",
    "    #print(E_bar[:,:,p])\n",
    "    cur_val = np.min(E_bar[:,:,p], axis = 0)\n",
    "    \n",
    "    #print(cur_val)\n",
    "    cur_k = np.argmin(E_bar[:,:,p], axis = 0)\n",
    "\n",
    "    #print(cur_k)\n",
    "    \n",
    "    \n",
    "    dp_backtracking = cur_k[:,np.newaxis]\n",
    "\n",
    "    #print(dp_backtracking.shape)\n",
    "    p = p - 1\n",
    "    while( p >= 0 ):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * w * V(drange)[:,:,np.newaxis])\n",
    "        \n",
    "        #print(candidate_arrays.shape)\n",
    "        \n",
    "        # The cost new array gets the value of candidate_array[:,cur_k[i],:i]\n",
    "        # need to get the index of the minimum value i\n",
    "        New_Cost_array = candidate_arrays[:,cur_k[np.arange(0, im_left.shape[0], dtype=int)],np.arange(0, im_left.shape[0], dtype=int)]\n",
    "        #print(New_Cost_array.shape)\n",
    "        \n",
    "        cur_val = np.min(New_Cost_array, axis = 0)\n",
    "    \n",
    "        new_k = np.argmin(New_Cost_array, axis = 0)\n",
    "        #print(new_k.shape)\n",
    " \n",
    "        dp_backtracking = np.hstack((new_k[:,np.newaxis],dp_backtracking))\n",
    "        p = p - 1\n",
    "    return dp_backtracking\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got no idea how to not iterate through all pixels in the image. I am sorry and I tried my best. The viterbi algorithem showed in class fits the shema of a for loop. I got no idea how to not use a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi(im_left, im_right, d_min, d_max, 0)\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi(im_left, im_right, d_min, d_max, 10000)\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7(b)\n",
    "\n",
    "Test the case where the photoconsistency term $D_p(d)$ is computed by averaging $SD$ in a small window of size $h$. That is, for each disparity $d$, you should replace the 2D array $\\text{SD\\_array}(d)$ in Problem 7(a) by  \n",
    "\n",
    "$$\n",
    "\\text{SD\\_array}(d) \\;\\longleftarrow\\; \\text{windSum}(SD(d), h)\n",
    "$$\n",
    "\n",
    "Compare the results for different window sizes $h \\in \\{1, 3, 5\\}$. Feel free to fine-tune the regularization parameter $w$ for each case, trying to obtain the best results possible.  \n",
    "\n",
    "**NOTE 1:**  \n",
    "$h = 1$ should be equivalent to Problem 7(a) above.  \n",
    "\n",
    "**NOTE 2:**  \n",
    "This version combines window-based stereo with regularization along scan lines. The case when $w = 0$ should give the same results as in Problem 6(c).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi_7b(im_left, im_right, d_min, d_max, 3,3)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "\n",
    "\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LinesMap = viterbi_7b(im_left, im_right, d_min, d_max, 1,10)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "\n",
    "\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8 (aligning disparity boundaries with intensity edges): test the following case where regularization weights $w_{pq}$ depend on a specific pair of neighboring points $$V_{pq}(d_p,d_q) = w_{pq}|d_p-d_q|$$ rather than all being equal to one constant $w_{pq}=w$, as in Problem 7. For such locally adaptive regularization weights $w_{pq}$ it is common to use local intensity contrast (in the reference image)  $$w_{pq}=w\\exp\\frac{-\\|I_p-I_q\\|^2}{2\\sigma^2}$$ which weighs the overall regularization constant $w$ by a Gaussian kernel (in RGB or grey-scale space). The latter makes it cheaper to draw large disparity jumps at \"contrast edges\" or \"contrast boundaries\" that are likely to happen at object boundaries. Note that bandwith parameter $\\sigma$ is important - it controls sensitivity to contrast edges. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_arr_mult(m,c):\n",
    "     (m.T * c).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bandwidth param is equivalent to sigma\n",
    "\n",
    "def wpq(drange, SD_array_image, w, p, bandwidth_param):\n",
    "    empty_grid = np.ogrid[0:drange,0:drange]\n",
    "    i = empty_grid[0]\n",
    "    k = empty_grid[1]\n",
    "    return w*np.exp(-SD_array_image[k, :, p][0]/(2 * bandwidth_param * bandwidth_param) )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_8(im_left, im_right, d_min, d_max, w , window_size, bandwidth_param):\n",
    "    drange = d_max - d_min + 1\n",
    "    # the depth, the pixels and the other rows in the image\n",
    "    E_bar = np.zeros((drange,im_left.shape[0],im_left.shape[1]))\n",
    "    #print(E_bar.shape)\n",
    "    SD_array_image = SD_array(im_left, im_right, d_min, d_max)\n",
    "\n",
    "\n",
    "    for p in range(1,im_left.shape[1]):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        #candidate_arrays = E_bar[i,p - 1] + (SD_array_image[k, row, p] + w * np.abs(k - i))\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        V_arr = np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * V(drange)[:,:,np.newaxis]\n",
    "        wpq_arr = wpq(drange, SD_array_image, w, p, bandwidth_param)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)        \n",
    "        V_arr =np.einsum('ijk,jk->jik',V_arr, wpq_arr)\n",
    "        \n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + V_arr)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        # One need to take the minimum value along axis 0 to get the minimum values of i that needs to be taken for each k\n",
    "        E_bar[k, :, p] = np.min(candidate_arrays,axis = 0)    \n",
    "    \n",
    "    #print(E_bar)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    p = im_left.shape[1] - 1\n",
    "\n",
    "    #print(E_bar[:,:,p])\n",
    "    cur_val = np.min(E_bar[:,:,p], axis = 0)\n",
    "    \n",
    "    #print(cur_val)\n",
    "    cur_k = np.argmin(E_bar[:,:,p], axis = 0)\n",
    "\n",
    "    #print(cur_k)\n",
    "    \n",
    "    \n",
    "    dp_backtracking = cur_k[:,np.newaxis]\n",
    "\n",
    "    #print(dp_backtracking.shape)\n",
    "    p = p - 1\n",
    "    while( p >= 0 ):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        V_arr = np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * V(drange)[:,:,np.newaxis]\n",
    "        wpq_arr = wpq(drange, SD_array_image, w, p, bandwidth_param)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)        \n",
    "        V_arr =np.einsum('ijk,jk->jik',V_arr, wpq_arr)\n",
    "        \n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + V_arr)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)\n",
    "        \n",
    "        #print(wpq_arr.shape)\n",
    "        \n",
    "        # The cost new array gets the value of candidate_array[:,cur_k[i],:i]\n",
    "        # need to get the index of the minimum value i\n",
    "        New_Cost_array = candidate_arrays[:,cur_k[line_scan_ind],line_scan_ind]\n",
    "        #print(New_Cost_array.shape)\n",
    "        \n",
    "        cur_val = np.min(New_Cost_array, axis = 0)\n",
    "    \n",
    "        new_k = np.argmin(New_Cost_array, axis = 0)\n",
    "        #print(new_k.shape)\n",
    " \n",
    "        dp_backtracking = np.hstack((new_k[:,np.newaxis],dp_backtracking))\n",
    "        p = p - 1\n",
    "    return dp_backtracking\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi_8(im_left, im_right, d_min, d_max, 5,1,190)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi_8(im_left, im_right, d_min, d_max, 5,1,19)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi_8(im_left, im_right, d_min, d_max, 1,1,9)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9 [optional, small bonus]: test the performance for quadratic and (robust) truncated-quadratic regularization terms $$V_{pq}(d_p,d_q) = w_{pq}|d_p-d_q|^2\\;\\;\\;\\text{and}\\;\\;\\;\\;V_{pq}(d_p,d_q) = w_{pq}\\, min\\{|d_p-d_q|^2,T\\}.$$ Discuss the differences in the results, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_9a(im_left, im_right, d_min, d_max, w , window_size):\n",
    "    drange = d_max - d_min + 1\n",
    "    # the depth, the pixels and the other rows in the image\n",
    "    E_bar = np.zeros((drange,im_left.shape[0],im_left.shape[1]))\n",
    "    #print(E_bar.shape)\n",
    "    SD_array_image = SD_array(im_left, im_right, d_min, d_max)\n",
    "\n",
    "\n",
    "    for p in range(1,im_left.shape[1]):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        #candidate_arrays = E_bar[i,p - 1] + (SD_array_image[k, row, p] + w * np.abs(k - i))\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        V_arr = np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * V(drange)[:,:,np.newaxis]\n",
    "        # wpq_arr = wpq(drange, SD_array_image, w, p, bandwidth_param)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)        \n",
    "        V_arr = V_arr * V_arr\n",
    "        \n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + V_arr)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        # One need to take the minimum value along axis 0 to get the minimum values of i that needs to be taken for each k\n",
    "        E_bar[k, :, p] = np.min(candidate_arrays,axis = 0)    \n",
    "    \n",
    "    #print(E_bar)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    p = im_left.shape[1] - 1\n",
    "\n",
    "    #print(E_bar[:,:,p])\n",
    "    cur_val = np.min(E_bar[:,:,p], axis = 0)\n",
    "    \n",
    "    #print(cur_val)\n",
    "    cur_k = np.argmin(E_bar[:,:,p], axis = 0)\n",
    "\n",
    "    #print(cur_k)\n",
    "    \n",
    "    \n",
    "    dp_backtracking = cur_k[:,np.newaxis]\n",
    "\n",
    "    #print(dp_backtracking.shape)\n",
    "    p = p - 1\n",
    "    while( p >= 0 ):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        V_arr = np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * V(drange)[:,:,np.newaxis]\n",
    "        # wpq_arr = wpq(drange, SD_array_image, w, p, bandwidth_param)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)        \n",
    "        V_arr = V_arr * V_arr\n",
    "        # V_arr =np.einsum('ijk,jk->jik',V_arr, wpq_arr)\n",
    "        \n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + V_arr)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)\n",
    "        \n",
    "        #print(wpq_arr.shape)\n",
    "        \n",
    "        # The cost new array gets the value of candidate_array[:,cur_k[i],:i]\n",
    "        # need to get the index of the minimum value i\n",
    "        New_Cost_array = candidate_arrays[:,cur_k[line_scan_ind],line_scan_ind]\n",
    "        #print(New_Cost_array.shape)\n",
    "        \n",
    "        cur_val = np.min(New_Cost_array, axis = 0)\n",
    "    \n",
    "        new_k = np.argmin(New_Cost_array, axis = 0)\n",
    "        #print(new_k.shape)\n",
    " \n",
    "        dp_backtracking = np.hstack((new_k[:,np.newaxis],dp_backtracking))\n",
    "        p = p - 1\n",
    "    return dp_backtracking\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi_9a(im_left, im_right, d_min, d_max, 1,1,)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_9b(im_left, im_right, d_min, d_max, w , window_size, T):\n",
    "    drange = d_max - d_min + 1\n",
    "    # the depth, the pixels and the other rows in the image\n",
    "    E_bar = np.zeros((drange,im_left.shape[0],im_left.shape[1]))\n",
    "    #print(E_bar.shape)\n",
    "    SD_array_image = SD_array(im_left, im_right, d_min, d_max)\n",
    "\n",
    "\n",
    "    for p in range(1,im_left.shape[1]):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        #candidate_arrays = E_bar[i,p - 1] + (SD_array_image[k, row, p] + w * np.abs(k - i))\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        V_arr = np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * V(drange)[:,:,np.newaxis]\n",
    "        # wpq_arr = wpq(drange, SD_array_image, w, p, bandwidth_param)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)      \n",
    "        V_arr = np.minimum(np.ones(V_arr.shape) * T, V_arr * V_arr)\n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + V_arr)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        # One need to take the minimum value along axis 0 to get the minimum values of i that needs to be taken for each k\n",
    "        E_bar[k, :, p] = np.min(candidate_arrays,axis = 0)    \n",
    "    \n",
    "    #print(E_bar)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    p = im_left.shape[1] - 1\n",
    "\n",
    "    #print(E_bar[:,:,p])\n",
    "    cur_val = np.min(E_bar[:,:,p], axis = 0)\n",
    "    \n",
    "    #print(cur_val)\n",
    "    cur_k = np.argmin(E_bar[:,:,p], axis = 0)\n",
    "\n",
    "    #print(cur_k)\n",
    "    \n",
    "    \n",
    "    dp_backtracking = cur_k[:,np.newaxis]\n",
    "\n",
    "    #print(dp_backtracking.shape)\n",
    "    p = p - 1\n",
    "    while( p >= 0 ):\n",
    "        empty_grid = np.ogrid[0:drange,0:drange]\n",
    "        i = empty_grid[0]\n",
    "        k = empty_grid[1]\n",
    "        # the new_cost_array is the array in which the potential new costs are stored\n",
    "        # it is equivalent to the red lines\n",
    "        # The candidate_arrays are the variouse costs assoiciated with setting the disparity value to be k \n",
    "        # where the last node have disparity value i\n",
    "        # In this case I need to have the third row which is the number of line scans there are\n",
    "        V_arr = np.ones(im_left.shape[0])[np.newaxis,np.newaxis, :] * V(drange)[:,:,np.newaxis]\n",
    "        # wpq_arr = wpq(drange, SD_array_image, w, p, bandwidth_param)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)        \n",
    "        V_arr = np.minimum(np.ones(V_arr.shape) * T, V_arr * V_arr)\n",
    "        # V_arr =np.einsum('ijk,jk->jik',V_arr, wpq_arr)\n",
    "        \n",
    "        candidate_arrays = E_bar[i, :, p - 1] + (SD_array_image[k, :, p] + V_arr)\n",
    "        line_scan_ind = np.arange(0, im_left.shape[0], dtype=int)\n",
    "        \n",
    "        #print(wpq_arr.shape)\n",
    "        \n",
    "        # The cost new array gets the value of candidate_array[:,cur_k[i],:i]\n",
    "        # need to get the index of the minimum value i\n",
    "        New_Cost_array = candidate_arrays[:,cur_k[line_scan_ind],line_scan_ind]\n",
    "        #print(New_Cost_array.shape)\n",
    "        \n",
    "        cur_val = np.min(New_Cost_array, axis = 0)\n",
    "    \n",
    "        new_k = np.argmin(New_Cost_array, axis = 0)\n",
    "        #print(new_k.shape)\n",
    " \n",
    "        dp_backtracking = np.hstack((new_k[:,np.newaxis],dp_backtracking))\n",
    "        p = p - 1\n",
    "    return dp_backtracking\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinesMap = viterbi_9b(im_left, im_right, d_min, d_max, 1,1,5)\n",
    "empty_grid = np.ogrid[0:im_left.shape[0]]\n",
    "plt.title(\"ground truth disparity map \")\n",
    "plt.imshow(LinesMap, vmin = 0, vmax = d_max)\n",
    "plt.colorbar(cax=plt.axes([0.91, 0.3, 0.01, 0.4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first option highly peanalizes the disparity change the disparity differences between the pixels within a scanline are small. However the existance of streaklines are very noticable. Whereas the second image have lower differences peanilized on a quadratic scale and higher differences as a constant T. Thus it is giving a level of tolorance to higher levels of disparity differences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
